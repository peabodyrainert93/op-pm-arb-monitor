# token_registry_core.py
import os
import json
import re
import time
import random
import threading
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urlparse, parse_qs

import requests
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

load_dotenv()

SCHEMA_VERSION = 6  # v6ï¼šä¿®æ­£ categorical ä¸‹æ¯ä¸ª polymarket å­å¸‚åœº endDateï¼ˆæŒ‰ market çº§åˆ«å†™å…¥ï¼‰

# requests è¶…æ—¶ï¼š(connect_timeout, read_timeout)
# æ—§ç‰ˆ read_timeout=10 åœ¨å¹¶å‘æ—¶æ›´å®¹æ˜“è§¦å‘ Read timed outï¼Œè¿™é‡Œç»™æ›´å®½æ¾çš„é»˜è®¤å€¼ã€‚
HTTP_TIMEOUT = (6, 25)

OPINION_BASE_URL = "https://openapi.opinion.trade/openapi"
GAMMA_BASE_URL = "https://gamma-api.polymarket.com"

def _parse_opinion_keys() -> List[str]:
    """æ”¯æŒå¤šæ¡ Opinion API Keyï¼š

    - æ¨èï¼šOPINION_API_KEYS=key1,key2,key3
    - å…¼å®¹æ—§å†™æ³•ï¼šOPINION_API_KEY=key1
    """
    raw = (os.getenv("OPINION_API_KEYS") or "").strip()
    if raw:
        keys = [k.strip() for k in raw.split(",") if k.strip()]
        return keys
    legacy = (os.getenv("OPINION_API_KEY") or "").strip()
    return [legacy] if legacy else []


OPINION_API_KEYS: List[str] = _parse_opinion_keys()


class TokenFetcherError(Exception):
    pass


# =========================
# Thread-local Session (çº¿ç¨‹å®‰å…¨)
# =========================
_thread_local = threading.local()


def _get_session() -> requests.Session:
    if getattr(_thread_local, "session", None) is None:
        s = requests.Session()
        _thread_local.session = s
    return _thread_local.session


# =========================
# Per-host Rate Limiter
# =========================
def _env_float(name: str, default: float) -> float:
    v = os.getenv(name, "").strip()
    if not v:
        return default
    try:
        return float(v)
    except Exception:
        return default


def _env_int(name: str, default: int) -> int:
    v = os.getenv(name, "").strip()
    if not v:
        return default
    try:
        return int(v)
    except Exception:
        return default


class HostRateLimiter:
    def __init__(self, host_min_interval: Dict[str, float]):
        self.host_min_interval = host_min_interval
        self._lock = threading.Lock()
        self._next_allowed: Dict[str, float] = {}

    def wait(self, url: str):
        host = urlparse(url).netloc.lower()
        interval = self.host_min_interval.get(host, 0.0)
        if interval <= 0:
            return

        now = time.monotonic()
        with self._lock:
            nxt = self._next_allowed.get(host, now)
            scheduled = max(now, nxt)
            self._next_allowed[host] = scheduled + interval

        sleep_s = scheduled - now
        if sleep_s > 0:
            time.sleep(sleep_s)


DEFAULT_OPINION_MIN_INTERVAL = _env_float("OPINION_MIN_INTERVAL", 0.25)
DEFAULT_GAMMA_MIN_INTERVAL = _env_float("GAMMA_MIN_INTERVAL", 0.25)

# è¯·æ±‚è¶…æ—¶ï¼šrequests æ”¯æŒ (connect_timeout, read_timeout)
# ä½ å¯ä»¥åœ¨ .env é‡Œè®¾ç½®ï¼šHTTP_CONNECT_TIMEOUT / HTTP_READ_TIMEOUT
HTTP_CONNECT_TIMEOUT = _env_float("HTTP_CONNECT_TIMEOUT", 6.0)
HTTP_READ_TIMEOUT = _env_float("HTTP_READ_TIMEOUT", 20.0)
HTTP_TIMEOUT = (HTTP_CONNECT_TIMEOUT, HTTP_READ_TIMEOUT)

_RATE_LIMITER = HostRateLimiter(
    {
        "openapi.opinion.trade": DEFAULT_OPINION_MIN_INTERVAL,
        "gamma-api.polymarket.com": DEFAULT_GAMMA_MIN_INTERVAL,
    }
)

MAX_WORKERS = _env_int("MAX_WORKERS", 8)

MAX_RETRIES = _env_int("HTTP_MAX_RETRIES", 4)
BACKOFF_BASE = _env_float("HTTP_BACKOFF_BASE", 0.6)


# =========================
# Common helpers
# =========================
def _require_opinion_key():
    if not OPINION_API_KEYS:
        raise TokenFetcherError("æœªé…ç½® Opinion API keyï¼šè¯·åœ¨ .env ä¸­æ·»åŠ  OPINION_API_KEY æˆ– OPINION_API_KEYS")


def _pick_opinion_key() -> str:
    """çº¿ç¨‹å†…è½®è¯¢é€‰æ‹© keyï¼Œé¿å…æ‰€æœ‰çº¿ç¨‹éƒ½æ‰“åˆ°åŒä¸€æ¡ keyã€‚"""
    if not OPINION_API_KEYS:
        return ""
    if len(OPINION_API_KEYS) == 1:
        return OPINION_API_KEYS[0]
    idx = getattr(_thread_local, "op_key_idx", 0)
    k = OPINION_API_KEYS[idx % len(OPINION_API_KEYS)]
    setattr(_thread_local, "op_key_idx", idx + 1)
    return k


def _opinion_headers() -> Dict[str, str]:
    _require_opinion_key()
    return {"apikey": _pick_opinion_key(), "Accept": "application/json"}


def extract_opinion_market_id_from_url(opinion_url: str) -> int:
    parsed = urlparse(opinion_url)
    qs = parse_qs(parsed.query)
    for key in ("topicId", "marketId", "id"):
        if key in qs and qs[key]:
            try:
                return int(qs[key][0])
            except ValueError:
                pass
    raise TokenFetcherError(f"æ— æ³•ä» Opinion URL è§£æ marketId: {opinion_url}")


def extract_polymarket_slug_from_url(poly_url: str) -> str:
    parsed = urlparse(poly_url)
    parts = parsed.path.strip("/").split("/")
    if not parts:
        raise TokenFetcherError(f"æ— æ³•ä» Polymarket URL è§£æ slug: {poly_url}")
    return parts[-1]


def _norm_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
    s = s.strip('"').strip("'")
    # æŠŠå„ç§ç®­å¤´ç»Ÿä¸€æˆç©ºæ ¼ï¼ˆkey é‡Œç”¨ digits-only/word-only å†è¡¥ï¼‰
    s = s.replace("â†‘", " ").replace("â†“", " ").replace("â†’", " ").replace("â€“", "-")
    s = re.sub(r"[^a-z0-9\s]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _digits_only(s: str) -> str:
    return re.sub(r"\D+", "", s or "")


def _strip_years(s: str) -> str:
    # å»æ‰ 4 ä½å¹´ä»½ï¼ˆ1900-2099ï¼‰
    s = re.sub(r"\b(19|20)\d{2}\b", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _strip_rate_words(s: str) -> str:
    """
    æŠŠ "decrease rates" / "increase interest rates" è¿™ç±»å°¾å·´å»æ‰ï¼Œ
    è®©å®ƒèƒ½åŒ¹é…åˆ° "decrease" / "increase"
    """
    s2 = (s or "").strip()
    # æ³¨æ„ï¼šè¿™é‡Œå¤„ç†çš„æ˜¯â€œå·²ç» norm è¿‡â€çš„æ–‡æœ¬ï¼ˆå…¨å°å†™ã€æ— æ ‡ç‚¹ï¼‰
    s2 = re.sub(r"\binterest\s+rates?\b", " ", s2)
    s2 = re.sub(r"\brates?\b", " ", s2)
    s2 = re.sub(r"\brate\b", " ", s2)
    s2 = re.sub(r"\s+", " ", s2).strip()
    return s2

def _expand_compact_number_to_int(s: str) -> Optional[int]:
    """
    æŠŠ 150k / 1.5k / 2m / 0.25m / 1b è§£ææˆæ•´æ•°ï¼š
      k=1_000, m=1_000_000, b=1_000_000_000
    è¿”å› None è¡¨ç¤ºæ— æ³•è§£æ
    """
    if not s:
        return None
    t = s.strip().lower()
    # å»æ‰è´§å¸ç¬¦å·å’Œé€—å·
    t = t.replace("$", "").replace(",", "").replace("ï¼Œ", "")
    m = re.search(r"\b(\d+(?:\.\d+)?)\s*([kmb])\b", t)
    if not m:
        return None
    num = float(m.group(1))
    suf = m.group(2)
    mul = {"k": 1_000, "m": 1_000_000, "b": 1_000_000_000}[suf]
    return int(round(num * mul))


_MONTHS = (
    "january","february","march","april","may","june",
    "july","august","september","october","november","december",
)


def _extract_month_day(text: str) -> Optional[str]:
    t = (text or "").strip().lower()
    m = re.search(r"\b(" + "|".join(_MONTHS) + r")\s+(\d{1,2})\b", t)
    if not m:
        return None
    return f"{m.group(1)} {int(m.group(2))}"


def _extract_range(text: str) -> Optional[str]:
    t = (text or "").replace(",", "")
    m = re.search(r"(\d[\d]*)\s*(?:-|â€“|to)\s*(\d[\d]*)", t, flags=re.IGNORECASE)
    if not m:
        return None
    a = m.group(1)
    b = m.group(2)
    if a and b:
        return f"{a}-{b}"
    return None


def _extract_directional_threshold(text: str) -> Optional[Tuple[str, int]]:
    """ä»æ–‡æœ¬ä¸­å°è¯•æå–æ–¹å‘(ge/le/gt/lt) + æ•°å­—ã€‚

    æ”¯æŒï¼š
      - ç¬¦å·ï¼š>= <= > <
      - è¯ï¼šreach/hit/at least/above/over  vs  dip/below/under/less than
      - ç®­å¤´ï¼šâ†‘ / â†“
    """
    raw = (text or "")
    t = raw.replace(",", "")

    # 1) ç¬¦å·ä¼˜å…ˆ
    m = re.search(r"(>=|<=|>|<)\s*\$?\s*(\d+(?:\.\d+)?)", t)
    if m:
        op = m.group(1)
        num = float(m.group(2))
        if op == ">=":
            return ("ge", int(round(num)))
        if op == "<=":
            return ("le", int(round(num)))
        if op == ">":
            return ("gt", int(round(num)))
        if op == "<":
            return ("lt", int(round(num)))

    # 2) å…ˆæŠ“ä¸€ä¸ªâ€œåƒä»·æ ¼/é˜ˆå€¼â€çš„æ•°å­—
    num_s = None
    m2 = re.search(r"\$\s*(\d{2,}(?:\.\d+)?)", t)
    if m2:
        num_s = m2.group(1)
    else:
        m3 = re.search(r"\b(\d{2,}(?:\.\d+)?)\b", t)
        if m3:
            num_s = m3.group(1)
    if not num_s:
        return None
    num_i = int(round(float(num_s)))

    tl = t.lower()

    # 3) ç®­å¤´/æ–¹å‘è¯
    if "â†‘" in raw or re.search(r"\b(up|reach|hit|at least|above|over|greater than|more than)\b", tl):
        return ("ge", num_i)
    if "â†“" in raw or re.search(r"\b(down|dip|below|under|less than|at most)\b", tl):
        return ("le", num_i)

    return None


def _make_keys(label: str, extra_text: Optional[str] = None) -> List[str]:
    """ç»™ä¸€ä¸ª candidate ç”Ÿæˆå¤šç»„å¯åŒ¹é… keyï¼ˆæ›´å¼ºå¥ï¼‰ã€‚"""
    keys = set()
    if not label and not extra_text:
        return []

    texts = []
    if label:
        texts.append(label)
    if extra_text:
        texts.append(extra_text)

    for raw in texts:
        raw = (raw or "").strip()
        if not raw:
            continue

        # 1) å¸¸è§„ & å»é€—å·
        base = _norm_text(raw)
        if base:
            keys.add(base)

        no_comma = _norm_text(raw.replace(",", "").replace("ï¼Œ", ""))
        if no_comma:
            keys.add(no_comma)

        # 2) å»å¹´ä»½ï¼ˆDecember 15, 2025 -> December 15ï¼‰
        for k0 in (base, no_comma):
            if k0:
                k = _strip_years(k0)
                if k:
                    keys.add(k)

        # 3) å» rates/interest rates
        for k0 in list(keys):
            k1 = _strip_rate_words(k0)
            if k1:
                keys.add(k1)

        # 4) æœˆ-æ—¥
        md = _extract_month_day(raw)
        if md:
            keys.add(md)

        # 5) æ•°å­—/ç´§å‡‘æ•°å­—
        digits = _digits_only(raw)
        if len(digits) >= 3:
            keys.add(digits)

        expanded = _expand_compact_number_to_int(raw)
        if expanded is not None and expanded >= 100:
            keys.add(str(expanded))

        # 6) åŒºé—´ï¼ˆ280â€“295 -> 280-295ï¼‰
        rg = _extract_range(raw)
        if rg:
            keys.add(rg)

        # 7) æ–¹å‘é˜ˆå€¼ï¼ˆâ†‘105k / below 4000 / >$500 ç­‰ï¼‰
        th = _extract_directional_threshold(raw)
        if th:
            op, num = th
            keys.add(f"{op}_{num}")
            keys.add(str(num))

    # 8) Increase/Decrease ç­‰å€™é€‰ï¼ˆå…¼å®¹ "Decrease rates" ç­‰ï¼‰
    for k0 in list(keys):
        m = re.fullmatch(r"(increase|decrease)(\s+rates?)?", k0)
        if m:
            keys.add(m.group(1))

        # hold / no change / unchanged
        if k0 in {"hold", "no change", "unchanged", "nochange"}:
            keys.add("hold")
            keys.add("no change")

    # 9) åŒä¹‰ï¼ˆanother/otherï¼‰
    if "another game" in keys or "another" in keys:
        keys.add("other")
    if "other" in keys:
        keys.add("another game")

    return [k for k in keys if k]


def _key_weight(k: str) -> int:
    k = (k or "").strip()
    if not k:
        return 0
    if re.match(r"^(ge|gt|le|lt)_\d{2,}$", k):
        return 12
    if re.match(r"^\d{3,}$", k):
        return 10
    if re.match(r"^\d{2,}-\d{2,}$", k):
        return 9
    if re.match(r"^(" + "|".join(_MONTHS) + r")\s+\d{1,2}$", k):
        return 7
    if k in {"increase", "decrease", "hold", "no change"}:
        return 3
    if k in {"yes", "no"}:
        return 2
    return 1


def _score_keys(keys: set) -> int:
    return sum(_key_weight(k) for k in keys)



def _request_with_retry(method: str, url: str, *, headers=None, params=None, timeout=HTTP_TIMEOUT) -> requests.Response:
    sess = _get_session()
    last_exc = None
    retryable_status = {429, 500, 502, 503, 504}

    for attempt in range(MAX_RETRIES):
        try:
            _RATE_LIMITER.wait(url)
            resp = sess.request(method, url, headers=headers, params=params, timeout=timeout)

            if resp.status_code == 200:
                return resp

            if resp.status_code == 403 and attempt == 0:
                time.sleep(BACKOFF_BASE + random.uniform(0, 0.25))
                continue

            if resp.status_code in retryable_status:
                time.sleep((BACKOFF_BASE * (2 ** attempt)) + random.uniform(0, 0.25))
                continue

            return resp

        except (requests.exceptions.RequestException, ConnectionError) as e:
            last_exc = e
            time.sleep((BACKOFF_BASE * (2 ** attempt)) + random.uniform(0, 0.25))

    if last_exc:
        raise TokenFetcherError(f"è¯·æ±‚å¤±è´¥ï¼ˆé‡è¯•åä»å¤±è´¥ï¼‰: {url} ; err={last_exc}")
    raise TokenFetcherError(f"è¯·æ±‚å¤±è´¥ï¼ˆé‡è¯•åä»å¤±è´¥ï¼‰: {url}")


def _http_get_json(url: str, headers=None, params=None, timeout=HTTP_TIMEOUT) -> Any:
    resp = _request_with_retry("GET", url, headers=headers, params=params, timeout=timeout)
    if resp.status_code != 200:
        raise TokenFetcherError(f"HTTP {resp.status_code} for {url}: {resp.text[:300]}")
    try:
        return resp.json()
    except Exception:
        raise TokenFetcherError(f"é JSON å“åº”: {url}: {resp.text[:300]}")


# =========================
# Cache helpers
# =========================
def _cache_key_from_cfg(cfg: Dict[str, str]) -> str:
    mtype = cfg.get("type", "binary")
    op_id = extract_opinion_market_id_from_url(cfg["opinion_url"])
    slug = cfg.get("polymarket_slug") or extract_polymarket_slug_from_url(cfg["polymarket_url"])
    return f"{mtype}|{op_id}|{slug}"


def _cache_key_from_entry(entry: Dict[str, Any]) -> Optional[str]:
    try:
        mtype = entry.get("type")
        if mtype == "binary":
            op_id = entry.get("opinion", {}).get("market_id")
            slug = entry.get("polymarket", {}).get("slug")
            if op_id and slug:
                return f"binary|{int(op_id)}|{slug}"
        elif mtype == "categorical":
            op_id = entry.get("opinion_market_id") or entry.get("opinion", {}).get("main_market_id")
            slug = entry.get("polymarket_event_slug") or entry.get("polymarket", {}).get("slug")
            if op_id and slug:
                return f"categorical|{int(op_id)}|{slug}"
    except Exception:
        return None
    return None


def load_cache(cache_path: str) -> Dict[str, Dict[str, Any]]:
    if not cache_path or not os.path.exists(cache_path):
        return {}
    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, list):
            return {}
    except Exception:
        return {}

    cache: Dict[str, Dict[str, Any]] = {}
    for entry in data:
        if not isinstance(entry, dict):
            continue
        k = _cache_key_from_entry(entry)
        if k:
            cache[k] = entry
    return cache


def _entry_is_usable(entry: Dict[str, Any]) -> bool:
    try:
        # âœ… schema ç‰ˆæœ¬ä¸å¯¹å°±å¼ºåˆ¶åˆ·æ–°
        if entry.get("schema_version") != SCHEMA_VERSION:
            return False

        if entry.get("type") == "binary":
            op = entry.get("opinion", {})
            pm = entry.get("polymarket", {})
            return bool(op.get("yes_token_id") and op.get("no_token_id") and pm.get("clob_token_ids"))

        if entry.get("type") == "categorical":
            return isinstance(entry.get("pairs"), list)

    except Exception:
        return False
    return False



# =========================
# Opinion
# =========================
def _opinion_get_market_generic(market_id: int) -> Dict[str, Any]:
    _require_opinion_key()
    headers = _opinion_headers()

    urls = [
        f"{OPINION_BASE_URL}/market/binary/{market_id}",
        f"{OPINION_BASE_URL}/market/{market_id}",
    ]

    last_err = None
    for url in urls:
        try:
            data = _http_get_json(url, headers=headers, timeout=HTTP_TIMEOUT)
        except Exception as e:
            last_err = str(e)
            continue

        code = data.get("code", data.get("errno"))
        if code not in (0, None):
            last_err = f"Opinion è¿”å›é”™è¯¯: {data}"
            continue

        result = data.get("result") or {}
        market = result.get("data") or result
        if market:
            return market

        last_err = f"Opinion è¿”å›ç»“æ„å¼‚å¸¸: {data}"

    raise TokenFetcherError(last_err or f"Opinion å¸‚åœºè¯¦æƒ…è·å–å¤±è´¥: {market_id}")


def _opinion_get_categorical_market_by_id(market_id: int) -> Dict[str, Any]:
    _require_opinion_key()
    url = f"{OPINION_BASE_URL}/market/categorical/{market_id}"
    headers = _opinion_headers()

    data = _http_get_json(url, headers=headers, timeout=HTTP_TIMEOUT)
    code = data.get("code", data.get("errno"))
    if code not in (0, None):
        raise TokenFetcherError(f"Opinion è¿”å›é”™è¯¯: {data}")

    result = data.get("result") or {}
    market = result.get("data") or result
    return market


def opinion_fetch_binary_tokens(market_id: int) -> Dict[str, Any]:
    market = _opinion_get_market_generic(market_id)
    return {
        "market_id": market.get("marketId") or market_id,
        "title": market.get("marketTitle") or "",
        "yes_token_id": market.get("yesTokenId"),
        "no_token_id": market.get("noTokenId"),
    }


def opinion_fetch_categorical_children(market_id: int) -> List[Dict[str, Any]]:
    market = _opinion_get_categorical_market_by_id(market_id)
    children = market.get("childMarkets") or []
    out: List[Dict[str, Any]] = []
    for c in children:
        out.append(
            {
                "market_id": c.get("marketId"),
                "title": c.get("marketTitle") or "",
                "yes_token_id": c.get("yesTokenId"),
                "no_token_id": c.get("noTokenId"),
            }
        )
    return out


# =========================
# Polymarket (Gamma)
# =========================
def _gamma_headers() -> Dict[str, str]:
    return {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "application/json",
    }


def _parse_json_list(v: Any) -> List[Any]:
    if isinstance(v, list):
        return v
    if isinstance(v, str):
        try:
            return json.loads(v)
        except Exception:
            return []
    return []


def gamma_get_event_by_slug(slug: str) -> Dict[str, Any]:
    headers = _gamma_headers()

    def get_event(s: str) -> Optional[Dict[str, Any]]:
        url = f"{GAMMA_BASE_URL}/events/slug/{s}"
        resp = _request_with_retry("GET", url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.json()
        if resp.status_code == 404:
            return None
        raise TokenFetcherError(f"Polymarket /events/slug/{s} HTTP {resp.status_code}: {resp.text[:300]}")

    m = re.search(r"(.+)-\d+$", slug)
    alt_slug = m.group(1) if m else None

    ev = get_event(slug)
    if ev:
        return ev

    if alt_slug:
        ev2 = get_event(alt_slug)
        if ev2:
            return ev2

    raise TokenFetcherError(f"Polymarket event slug '{slug}'ï¼ˆåŠå…¶ altï¼‰404 / ä¸å­˜åœ¨")


def gamma_get_market_by_slug_or_event(slug: str) -> Dict[str, Any]:
    headers = _gamma_headers()

    def get_market(s: str) -> Optional[Dict[str, Any]]:
        url = f"{GAMMA_BASE_URL}/markets/slug/{s}"
        resp = _request_with_retry("GET", url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.json()
        if resp.status_code == 404:
            return None
        raise TokenFetcherError(f"Polymarket /markets/slug/{s} HTTP {resp.status_code}: {resp.text[:300]}")

    def alt(s: str) -> Optional[str]:
        m = re.search(r"(.+)-\d+$", s)
        return m.group(1) if m else None

    m1 = get_market(slug)
    if m1:
        return m1

    a = alt(slug)
    if a:
        m2 = get_market(a)
        if m2:
            return m2

    # fallback: event -> pick first market
    ev = gamma_get_event_by_slug(slug)
    markets = ev.get("markets") or []
    if not markets:
        raise TokenFetcherError(f"events/slug/{slug} è¿”å›æˆåŠŸï¼Œä½† markets ä¸ºç©º")
    return markets[0]


def gamma_get_market_by_id(market_id: str) -> Dict[str, Any]:
    """æŒ‰ market id è·å–å®Œæ•´ marketï¼ˆç”¨äºè¡¥é½ endDate ç­‰å­—æ®µï¼‰ã€‚"""
    headers = _gamma_headers()
    mid = str(market_id)
    url = f"{GAMMA_BASE_URL}/markets/{mid}"
    resp = _request_with_retry("GET", url, headers=headers, timeout=HTTP_TIMEOUT)
    if resp.status_code == 200:
        return resp.json()
    raise TokenFetcherError(f"Polymarket /markets/{mid} HTTP {resp.status_code}: {resp.text[:300]}")


def gamma_parse_yes_no_from_market(m: Dict[str, Any]) -> Dict[str, Any]:
    outcomes = _parse_json_list(m.get("outcomes"))
    clob_ids = _parse_json_list(m.get("clobTokenIds"))

    yes_token_id = None
    no_token_id = None

    if len(outcomes) == 2 and len(clob_ids) == 2:
        o0 = str(outcomes[0]).strip().lower()
        o1 = str(outcomes[1]).strip().lower()

        # é»˜è®¤æŒ‰ [0]=YES [1]=NOï¼Œä½†å¦‚æœåè¿‡æ¥å°±äº¤æ¢
        if o0 == "no" and o1 == "yes":
            yes_token_id = str(clob_ids[1])
            no_token_id = str(clob_ids[0])
        else:
            # å¸¸è§æƒ…å†µï¼š["Yes","No"] æˆ– é yes/noï¼ˆUp/Downï¼‰å°±ä¿æŒ 0/1
            yes_token_id = str(clob_ids[0])
            no_token_id = str(clob_ids[1])

    return {
        "market_id": str(m.get("id") or ""),
        "question": m.get("question") or "",
        "yes_token_id": yes_token_id,
        "no_token_id": no_token_id,
        "outcomes": outcomes,
        "clob_token_ids": clob_ids,
    }


def _is_placeholder_candidate(name: str) -> bool:
    """
    æ›´é€šç”¨çš„å ä½è¿‡æ»¤ï¼š
      - Game C / Movie D / Team A / Option B / Candidate E ...
    """
    n = (name or "").strip()
    if bool(re.fullmatch(r"(Game|Movie|Team|Option|Candidate|Player|Item)\s+[A-Z]", n, flags=re.IGNORECASE)):
        return True

    # è¿™ç±»å…œåº•é¡¹ä¹Ÿé€šå¸¸ä¼šé€ æˆå™ªéŸ³/è¯¯åŒ¹é…
    if re.search(r"\banother game\b", n, re.IGNORECASE):
        return True

    return False

def gamma_extract_candidate_from_question(q: str) -> str:
    q = (q or "").strip()

    # ä¼˜å…ˆï¼šå•å¼•å·é‡Œçš„å€™é€‰é¡¹ï¼ˆSteam Awards è¿™ç§ï¼‰
    m = re.search(r"'([^']+)'", q)
    if m:
        return m.group(1).strip()

    # å…œåº•ï¼šanother game
    if re.search(r"\banother game\b", q, re.IGNORECASE):
        return "Another game"

    # æœ€åå…œåº•ï¼šè¿”å›åŸé—®é¢˜
    return q



def gamma_extract_candidate_from_market(raw_market: Dict[str, Any], question: str) -> str:
    """
    categorical ä¸‹ä¼˜å…ˆç”¨ groupItemTitleï¼ˆè¿™æ˜¯ä½ è¦çš„å…³é”®ï¼‰ï¼›
    æ²¡æœ‰ groupItemTitle å† fallback è§£æ questionã€‚
    """
    # 1) groupItemTitleï¼ˆä½ åœ¨æ–‡æ¡£é‡Œçœ‹åˆ°çš„ï¼‰
    for k in ("groupItemTitle", "group_item_title", "groupTitle", "group_item_title_text"):
        v = raw_market.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()

    q = (question or "").strip()

    # 2) åŸæ¥çš„ï¼šå¼•å·é‡Œçš„å€™é€‰ï¼ˆSteam Awards è¿™ç§ï¼‰
    m = re.search(r"'([^']+)'", q)
    if m:
        return m.group(1).strip()

    # 3) â€œanother gameâ€
    if re.search(r"\banother\b.*\bgame\b", q, re.IGNORECASE):
        return "Another game"

    # 4) åˆ©ç‡ Increase/Decrease/No change & bps
    if re.search(r"\bdecreases?\b|\bcuts?\b", q, re.IGNORECASE):
        m2 = re.search(r"by\s+(\d+\+?)\s*bps", q, re.IGNORECASE)
        return f"{m2.group(1)} bps decrease" if m2 else "Decrease"
    if re.search(r"\bincreases?\b|\bhikes?\b", q, re.IGNORECASE):
        m2 = re.search(r"by\s+(\d+\+?)\s*bps", q, re.IGNORECASE)
        return f"{m2.group(1)} bps increase" if m2 else "Increase"
    if re.search(r"\bno\s+change\b|\bunchanged\b|\bkeeps?\b", q, re.IGNORECASE):
        return "No change"

    # 5) close at range: $280â€“295
    m = re.search(r"close\s+at\s+\$?([0-9][0-9,]*)\s*(?:â€“|-|to)\s*\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        a = m.group(1).replace(",", "")
        b = m.group(2).replace(",", "")
        return f"${a}â€“{b}"

    # 6) close below/above: <$4000 / >$500
    m = re.search(r"close\s+below\s+\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        return f"<${m.group(1)}"
    m = re.search(r"close\s+above\s+\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        return f">${m.group(1)}"
    m = re.search(r"close\s+at\s+<\s*\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        return f"<${m.group(1)}"
    m = re.search(r"close\s+at\s+>\s*\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        return f">${m.group(1)}"

    # 7) hit/reach/dipï¼šâ†‘ 105,000 / â†“ 80,000 / â†‘$5,000
    m = re.search(r"(reach|hit)\s+\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        # å¸¦ $ çš„æ›´åƒ â†‘$5000
        if "$" in q[m.start():m.end()+1]:
            return f"â†‘${m.group(2)}"
        return f"â†‘ {m.group(2)}"
    m = re.search(r"dip\s+to\s+\$?([0-9][0-9,]*)", q, re.IGNORECASE)
    if m:
        return f"â†“ {m.group(1)}"

    # å…œåº•ï¼šå°±ç”¨ question
    return q or ""


def gamma_event_to_candidate_markets(event: Dict[str, Any]) -> Tuple[str, str, Optional[str], List[Dict[str, Any]]]:
    event_id = str(event.get("id") or "")
    event_title = event.get("title") or ""
    # äº‹ä»¶çº§ endDateï¼ˆæœ‰äº› event åªæœ‰ä¸€ä¸ªæ—¶é—´ï¼›ä½†ä¹Ÿå­˜åœ¨â€œåŒä¸€ event ä¸‹å¤šä¸ª market ä¸åŒ endDateâ€çš„æƒ…å†µï¼‰
    event_end_date = event.get("endDate")
    markets = event.get("markets") or []

    # events/slug æœ‰æ—¶ä¸ä¼šæŠŠæ¯ä¸ª market çš„ endDate å¸¦å…¨ï¼ˆä½† /markets/{id} ä¼šæœ‰ï¼‰ã€‚
    # ä¸ºäº†é¿å…åƒ TikTok è¿™ç§â€œåŒä¸€ event ä¸‹ä¸åŒ market ä¸åŒæˆªæ­¢æ—¶é—´â€è¢«å†™æˆåŒä¸€ä¸ªæ—¶é—´ï¼Œ
    # è¿™é‡ŒæŒ‰ market_id è¡¥é½ endDateã€‚
    enddate_cache: Dict[str, Optional[str]] = {}

    out: List[Dict[str, Any]] = []
    for m in markets:
        parsed = gamma_parse_yes_no_from_market(m)
        # âœ… ä¼˜å…ˆç”¨ groupItemTitleï¼ˆå®ƒé€šå¸¸å°±æ˜¯ç”¨æˆ·åœ¨å‰ç«¯çœ‹åˆ°çš„é€‰é¡¹æ–‡æ¡ˆï¼‰ï¼ŒåŒ¹é… Opinion çš„ candidate æ›´ç¨³ã€‚
        cand = (m.get("groupItemTitle") or m.get("group_item_title") or "").strip()
        if not cand:
            cand = gamma_extract_candidate_from_question(parsed.get("question") or "")
        parsed["candidate"] = cand

        # âœ… æ¯ä¸ª market è‡ªå·±çš„ endDateï¼ˆå¦‚æœ events/slug é‡Œç¼ºå¤±ï¼Œå°±æŒ‰ market_id å†æŸ¥ä¸€æ¬¡ /markets/{id}ï¼‰
        m_end = m.get("endDate") or m.get("end_date")
        if not m_end:
            mid = str(parsed.get("market_id") or "")
            if mid:
                if mid not in enddate_cache:
                    try:
                        full = gamma_get_market_by_id(mid)
                        enddate_cache[mid] = full.get("endDate") or full.get("end_date")
                    except Exception:
                        enddate_cache[mid] = None
                m_end = enddate_cache.get(mid)
        parsed["endDate"] = m_end or event_end_date

        # placeholder æ ‡è®°ï¼šåç»­ unmatched é»˜è®¤ä¸å†™å…¥ï¼Œå‡å°‘å™ªéŸ³
        parsed["placeholder"] = bool(
            _is_placeholder_candidate(parsed.get("candidate") or "")
            or re.search(r"\banother\s+game\b", parsed.get("candidate") or "", re.IGNORECASE)
        )

        # è¿‡æ»¤å ä½é¡¹ï¼šGame C / Game D / ...
        if parsed.get("placeholder"):
            continue

        out.append(parsed)

    return event_id, event_title, event_end_date, out



# =========================
# Builders
# =========================
def build_entry_from_urls(cfg: Dict[str, str]) -> Dict[str, Any]:
    name = cfg.get("name") or "UNNAMED"
    mtype = cfg.get("type", "binary")
    opinion_url = cfg["opinion_url"]
    poly_url = cfg["polymarket_url"]

    opinion_market_id = extract_opinion_market_id_from_url(opinion_url)
    slug = cfg.get("polymarket_slug") or extract_polymarket_slug_from_url(poly_url)

    if mtype == "binary":
        op = opinion_fetch_binary_tokens(opinion_market_id)
        pm_market = gamma_get_market_by_slug_or_event(slug)
        pm_parsed = gamma_parse_yes_no_from_market(pm_market)

        return {
            "schema_version": SCHEMA_VERSION,
            "name": name,
            "type": "binary",
            "opinion": {
                "market_id": op["market_id"],
                "yes_token_id": op["yes_token_id"],
                "no_token_id": op["no_token_id"],
            },
            "polymarket": {
                "slug": slug,
                "question": pm_parsed.get("question") or "",
                "outcomes": pm_parsed.get("outcomes") or _parse_json_list(pm_market.get("outcomes")),
                "clob_token_ids": pm_parsed.get("clob_token_ids") or _parse_json_list(pm_market.get("clobTokenIds")),
                "endDate": pm_market.get("endDate") or pm_market.get("end_date"),
            },
        }

    if mtype == "categorical":
        op_children = opinion_fetch_categorical_children(opinion_market_id)

        ev = gamma_get_event_by_slug(slug)
        ev_id, ev_title, ev_end_date, pm_candidate_markets = gamma_event_to_candidate_markets(ev)

        # Opinionï¼šæ¯ä¸ª candidate ç”Ÿæˆå¤šä¸ª keyï¼ˆtitle é‡Œç»å¸¸å°±æ˜¯é€‰é¡¹æ–‡æœ¬ï¼‰
        op_items: List[Dict[str, Any]] = []
        for c in op_children:
            title = c.get("title") or ""
            keys = set(_make_keys(title))
            op_items.append({"child": c, "keys": keys, "norm": _norm_text(title)})

        # Polymarketï¼šæå‰ç®—å¥½ key é›†åˆï¼Œæ–¹ä¾¿æ‰“åˆ†åŒ¹é…
        pm_items: List[Dict[str, Any]] = []
        for m in pm_candidate_markets:
            if m.get("placeholder"):
                continue
            cand = m.get("candidate") or ""
            q = m.get("question") or ""
            keys = set(_make_keys(cand, extra_text=q))
            pm_items.append({"m": m, "keys": keys, "norm": _norm_text(cand)})

        pairs: List[Dict[str, Any]] = []
        used_pm_market_ids: set = set()

        # é€ä¸ª opinion candidate æ‰¾â€œæœ€ä½³â€ polymarket å­å¸‚åœºï¼ˆæŒ‰ key overlap æ‰“åˆ†ï¼‰
        for it in op_items:
            oc = it["child"]
            best_pm: Optional[Dict[str, Any]] = None
            best_score = 0

            for cand in pm_items:
                pm = cand["m"]
                mid = pm.get("market_id")
                if not mid or mid in used_pm_market_ids:
                    continue
                if not (pm.get("yes_token_id") and pm.get("no_token_id")):
                    continue

                inter = it["keys"].intersection(cand["keys"])
                if not inter:
                    continue

                score = _score_keys(inter)
                # tie-breakï¼šå€™é€‰åå®Œå…¨ä¸€è‡´ä¼˜å…ˆ
                if score > best_score or (score == best_score and it["norm"] and it["norm"] == cand["norm"]):
                    best_score = score
                    best_pm = pm

            if best_pm:
                used_pm_market_ids.add(best_pm["market_id"])
                pairs.append(
                    {
                        "candidate": oc.get("title") or "",
                        "opinion": {
                            "market_id": oc.get("market_id"),
                            "yes_token_id": oc.get("yes_token_id"),
                            "no_token_id": oc.get("no_token_id"),
                        },
                        "polymarket": {
                            "market_id": best_pm.get("market_id"),
                            "question": best_pm.get("question") or "",
                            "candidate": best_pm.get("candidate") or "",
                            "yes_token_id": best_pm.get("yes_token_id"),
                            "no_token_id": best_pm.get("no_token_id"),
                            # âœ… é‡ç‚¹ï¼šæ¯ä¸ªå­å¸‚åœºä½¿ç”¨å®ƒè‡ªå·±çš„ endDateï¼›å¦‚æœç¼ºå¤±å†é€€å› event_endDate
                            "endDate": best_pm.get("endDate") or ev_end_date,
                        },
                    }
                )

        # unmatched opinion
        matched_op_ids = {p["opinion"]["market_id"] for p in pairs}
        unmatched_opinion: List[Dict[str, Any]] = []
        for c in op_children:
            if c.get("market_id") not in matched_op_ids:
                unmatched_opinion.append(
                    {
                        "market_id": c.get("market_id"),
                        "candidate": c.get("title") or "",
                        "yes_token_id": c.get("yes_token_id"),
                        "no_token_id": c.get("no_token_id"),
                    }
                )

        # unmatched polymarketï¼ˆé»˜è®¤ä¸å†™ placeholder=trueï¼‰
        unmatched_polymarket: List[Dict[str, Any]] = []
        for cand in pm_items:
            pm = cand["m"]
            mid = pm.get("market_id")
            if not mid or mid in used_pm_market_ids:
                continue
            if pm.get("placeholder"):
                continue
            if _is_placeholder_candidate(pm.get("candidate") or ""):
                continue
            unmatched_polymarket.append(
                {
                    "market_id": pm.get("market_id"),
                    "question": pm.get("question") or "",
                    "candidate": pm.get("candidate") or "",
                    "yes_token_id": pm.get("yes_token_id"),
                    "no_token_id": pm.get("no_token_id"),
                    "endDate": pm.get("endDate") or ev_end_date,
                }
            )

        return {
            "schema_version": SCHEMA_VERSION,              # âœ… é¡¶å±‚åŠ è¿™é‡Œ
            "name": name,
            "type": "categorical",
            "opinion_market_id": opinion_market_id,
            "polymarket_event_slug": slug,
            "polymarket_event_id": ev_id,
            "polymarket_event_title": ev_title,
            "polymarket_event_endDate": ev_end_date,       # âœ… é¡¶å±‚åŠ è¿™é‡Œ
            "pairs": pairs,
            "unmatched_opinion": unmatched_opinion,
            "unmatched_polymarket": unmatched_polymarket,
        }



    raise TokenFetcherError(f"ä¸æ”¯æŒçš„ type={mtype} (åªæ”¯æŒ binary / categorical)")


def build_all(
    url_pairs: List[Dict[str, str]],
    cache_path: Optional[str] = None,
    refresh: bool = False,
    keep_cache_on_error: bool = True,
) -> List[Dict[str, Any]]:
    cache: Dict[str, Dict[str, Any]] = load_cache(cache_path) if (cache_path and not refresh) else {}
    results_by_index: List[Optional[Dict[str, Any]]] = [None] * len(url_pairs)
    tasks: List[Tuple[int, str, Dict[str, str]]] = []

    for i, cfg in enumerate(url_pairs):
        name = cfg.get("name", "UNNAMED")

        try:
            key = _cache_key_from_cfg(cfg)
        except Exception as e:
            print(f"å¤„ç†ï¼š{name}")
            print(f"  âŒ å¤±è´¥ï¼šæ— æ³•ç”Ÿæˆç¼“å­˜ keyï¼š{e}\n")
            continue

        if (not refresh) and key in cache and _entry_is_usable(cache[key]):
            cached = cache[key]
            cached["name"] = name
            results_by_index[i] = cached
            print(f"å¤„ç†ï¼š{name}")
            print("  âœ… ä½¿ç”¨ç¼“å­˜ï¼ˆè·³è¿‡ API è¯·æ±‚ï¼‰\n")
        else:
            tasks.append((i, key, cfg))

    if not tasks:
        return [r for r in results_by_index if r is not None]

    print(f"=== å¹¶å‘æŠ“å–å¼€å§‹ï¼š{len(tasks)} ä¸ªä»»åŠ¡ï¼ŒMAX_WORKERS={MAX_WORKERS} ===\n")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        future_map = {}
        for (i, key, cfg) in tasks:
            future = ex.submit(build_entry_from_urls, cfg)
            future_map[future] = (i, key, cfg)

        for fut in as_completed(future_map):
            i, key, cfg = future_map[fut]
            name = cfg.get("name", "UNNAMED")

            try:
                entry = fut.result()
                results_by_index[i] = entry
                cache[key] = entry
                print(f"å¤„ç†ï¼š{name}")
                print("  âœ… æˆåŠŸï¼ˆå¹¶å‘æŠ“å– & å·²æ›´æ–°ç¼“å­˜ï¼‰\n")

            except Exception as e:
                if (not refresh) and keep_cache_on_error and key in cache and _entry_is_usable(cache[key]):
                    cached = cache[key]
                    cached["name"] = name
                    results_by_index[i] = cached
                    print(f"å¤„ç†ï¼š{name}")
                    print(f"  âš ï¸ æŠ“å–å¤±è´¥ä½†ä¿ç•™æ—§ç¼“å­˜ï¼š{e}\n")
                else:
                    print(f"å¤„ç†ï¼š{name}")
                    print(f"  âŒ å¤±è´¥ï¼š{e}\n")

    return [r for r in results_by_index if r is not None]

def _parse_iso_dt(s: Any) -> Optional[datetime]:
    if not s or not isinstance(s, str):
        return None
    t = s.strip()
    # å…¼å®¹ "Z"
    t = t.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(t)
    except Exception:
        return None
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)

def _collect_end_dts(entry: Dict[str, Any]) -> List[datetime]:
    """æ”¶é›†ä¸€ä¸ª entry é‡Œæ‰€æœ‰èƒ½æ‹¿åˆ°çš„ endDateï¼ˆbinary/categorical éƒ½æ”¯æŒï¼‰ã€‚"""
    dts: List[datetime] = []

    mtype = entry.get("type")
    if mtype == "binary":
        pm = entry.get("polymarket") or {}
        dt = _parse_iso_dt(pm.get("endDate") or pm.get("end_date"))
        if dt:
            dts.append(dt)

    elif mtype == "categorical":
        # é¡¶å±‚ event endDate
        dt0 = _parse_iso_dt(entry.get("polymarket_event_endDate") or entry.get("polymarket_event_end_date"))
        if dt0:
            dts.append(dt0)

        # pairs å­å¸‚åœº endDateï¼ˆä½  v6 å·²ç»æŒ‰ market çº§åˆ«å†™å…¥ï¼‰
        for p in (entry.get("pairs") or []):
            if not isinstance(p, dict):
                continue
            pm = p.get("polymarket") or {}
            dt = _parse_iso_dt(pm.get("endDate") or pm.get("end_date"))
            if dt:
                dts.append(dt)

        # unmatched_polymarket ä¹Ÿå¯èƒ½æœ‰ endDate
        for u in (entry.get("unmatched_polymarket") or []):
            if not isinstance(u, dict):
                continue
            dt = _parse_iso_dt(u.get("endDate") or u.get("end_date"))
            if dt:
                dts.append(dt)

    return dts

def prune_expired_markets(
    results: List[Dict[str, Any]],
    *,
    now_utc: Optional[datetime] = None,
    grace_seconds: float = 0.0,
    verbose: bool = True,
) -> List[Dict[str, Any]]:
    """
    - binaryï¼šè‹¥ polymarket.endDate å·²è¿‡æœŸ -> åˆ é™¤æ•´ä¸ª entry
    - categoricalï¼š
        1) å…ˆæŒ‰æ¯ä¸ªå­ market çš„ endDate è¿‡æ»¤ pairs / unmatched_polymarket
        2) è‹¥è¿‡æ»¤åæ²¡æœ‰ä»»ä½•å¯ç”¨ pairsï¼Œä¸”æ•´ä½“ latest endDate ä¹Ÿè¿‡æœŸ -> åˆ é™¤ entry
    """
    now_utc = now_utc or datetime.now(timezone.utc)
    grace = timedelta(seconds=float(grace_seconds or 0.0))

    kept: List[Dict[str, Any]] = []
    removed: List[Dict[str, Any]] = []
    removed_children_cnt = 0
    removed_entries_detail: List[Dict[str, Any]] = []
    removed_children_detail: List[Dict[str, Any]] = []

    for entry in results:
        if not isinstance(entry, dict):
            continue

        mtype = entry.get("type")
        # å…ˆå¯¹ categorical åšâ€œå­ market çº§åˆ«â€è¿‡æ»¤ï¼ˆæ›´ç¬¦åˆä½ è¯´çš„â€œåˆ é™¤è¿‡æœŸå¸‚åœºâ€ï¼‰
        if mtype == "categorical":
            # pairs
            new_pairs = []
            for p in (entry.get("pairs") or []):
                if not isinstance(p, dict):
                    continue
                pm = p.get("polymarket") or {}
                dt = _parse_iso_dt(pm.get("endDate") or pm.get("end_date"))
                if dt and (dt + grace) < now_utc:
                    removed_children_cnt += 1

                    # âœ… è®°å½•è¢«åˆ æ‰çš„ categorical å­å¸‚åœºï¼ˆpairsï¼‰
                    pm_mid = (pm.get("market_id") or pm.get("id") or "")
                    removed_children_detail.append({
                        "parent_name": entry.get("name", "UNNAMED"),
                        "parent_type": "categorical",
                        "parent_slug": entry.get("polymarket_event_slug") or (entry.get("polymarket") or {}).get("slug") or "",
                        "where": "pairs",
                        "pm_market_id": str(pm_mid),
                        "candidate": (p.get("candidate") or pm.get("candidate") or ""),
                        "endDate": (pm.get("endDate") or pm.get("end_date") or ""),
                    })

                    continue
                new_pairs.append(p)
            entry["pairs"] = new_pairs

            # unmatched_polymarket
            new_unmatched = []
            for u in (entry.get("unmatched_polymarket") or []):
                if not isinstance(u, dict):
                    continue
                dt = _parse_iso_dt(u.get("endDate") or u.get("end_date"))
                if dt and (dt + grace) < now_utc:
                    removed_children_cnt += 1

                    # âœ… è®°å½•è¢«åˆ æ‰çš„ categorical å­å¸‚åœºï¼ˆunmatched_polymarketï¼‰
                    removed_children_detail.append({
                        "parent_name": entry.get("name", "UNNAMED"),
                        "parent_type": "categorical",
                        "parent_slug": entry.get("polymarket_event_slug") or (entry.get("polymarket") or {}).get("slug") or "",
                        "where": "unmatched_polymarket",
                        "pm_market_id": str(u.get("market_id") or ""),
                        "candidate": (u.get("candidate") or ""),
                        "endDate": (u.get("endDate") or u.get("end_date") or ""),
                    })

                    continue
                new_unmatched.append(u)
            entry["unmatched_polymarket"] = new_unmatched

        # å†åˆ¤å®šâ€œæ•´ä¸ª entry æ˜¯å¦è¿‡æœŸâ€
        dts = _collect_end_dts(entry)
        latest = max(dts) if dts else None

        # è§„åˆ™ï¼šèƒ½æ‹¿åˆ° latest endDate ä¸” latest å·²è¿‡æœŸ => åˆ é™¤ entry
        if latest and (latest + grace) < now_utc:
            removed_entries_detail.append({
                "name": entry.get("name", "UNNAMED"),
                "type": entry.get("type", ""),
                "slug": (entry.get("polymarket") or {}).get("slug") or entry.get("polymarket_event_slug") or "",
                "latest_end": latest.isoformat(),
                "reason": "latest_endDate_expired",
            })
            removed.append(entry)
            continue

        # categoricalï¼šå¦‚æœ pairs å·²ç©ºï¼ˆæ²¡æœ‰å¯ç›‘æ§çš„ legsï¼‰ï¼Œä»…å½“æ•´ä½“ endDate å·²è¿‡æœŸæ‰ç§»é™¤ï¼ˆé¿å…è¯¯åˆ æœªè¿‡æœŸä½†æš‚æ—¶æ²¡åŒ¹é…åˆ°çš„ entryï¼‰
        if mtype == "categorical" and not (entry.get("pairs") or []):
            dts2 = _collect_end_dts(entry)
            latest2 = max(dts2) if dts2 else None

            if latest2 and (latest2 + grace) < now_utc:
                removed_entries_detail.append({
                    "name": entry.get("name", "UNNAMED"),
                    "type": entry.get("type", ""),
                    "slug": entry.get("polymarket_event_slug") or "",
                    "latest_end": latest2.isoformat() if latest2 else "NA",
                    "reason": "no_pairs_after_prune",
                })
                removed.append(entry)
                continue

        kept.append(entry)

    if verbose:
        print(f"ğŸ§¹ prune_expired: entry kept={len(kept)} removed={len(removed)}; removed_child_markets={removed_children_cnt}")
        # âœ… æ‰“å°åˆ é™¤çš„ entryï¼ˆé¡¶å±‚å¸‚åœºï¼‰
        if removed_entries_detail:
            print("ğŸ—‘ï¸ removed entries:")
            for d in removed_entries_detail:
                print(f"  - {d['name']} | {d['type']} | {d['slug']} | latest_end={d['latest_end']} | reason={d['reason']}")
        else:
            print("ğŸ—‘ï¸ removed entries: (none)")

        # âœ… æ‰“å°åˆ é™¤çš„ categorical å­å¸‚åœºï¼ˆpairs / unmatched_polymarketï¼‰
        if removed_children_detail:
            print("ğŸ—‘ï¸ removed child markets:")
            for c in removed_children_detail:
                print(
                    f"  - parent={c['parent_name']} | slug={c['parent_slug']} | where={c['where']} "
                    f"| pm_market_id={c['pm_market_id']} | candidate={c['candidate']} | endDate={c['endDate']}"
                )
        else:
            print("ğŸ—‘ï¸ removed child markets: (none)")

    return kept


def write_market_token_pairs_json(results: List[Dict[str, Any]], out_path: str) -> None:
    dir_ = os.path.dirname(out_path)
    if dir_:
        os.makedirs(dir_, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
